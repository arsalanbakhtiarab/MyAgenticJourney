{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages\n"
      ],
      "metadata": {
        "id": "VaY7UNE8k6xN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3TLhRWUydzWw"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily-python pydantic\n",
        "%pip install --quiet -U langgraph psycopg2-binary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages"
      ],
      "metadata": {
        "id": "7QS7VjInlmK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import psycopg2\n",
        "from pydantic import BaseModel\n",
        "from langgraph.graph import MessagesState\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage"
      ],
      "metadata": {
        "id": "D5av7O72k1Rm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Set up authentication keys"
      ],
      "metadata": {
        "id": "bXbL1MQ_mJjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "QzlvydKqe5Ol"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash\",\n",
        "    api_key = GEMINI_API_KEY,\n",
        "    temperature = 0)"
      ],
      "metadata": {
        "id": "u58PB6simTqw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"hi\")"
      ],
      "metadata": {
        "id": "9a__ykocm93F",
        "outputId": "ec885d17-3b43-473a-f063-a6013df01c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--170a8ef6-12fd-4981-a84f-81ce3f0aab53-0', usage_metadata={'input_tokens': 1, 'output_tokens': 11, 'total_tokens': 12, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ› ï¸ Define the tools"
      ],
      "metadata": {
        "id": "GsyCs_jqyEAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages (run these commands separately)\n",
        "# pip install langchain_google_genai langchain_core langchain_community pydantic\n",
        "# pip install langgraph psycopg2-binary sqlalchemy\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Setup Gemini LLM\n",
        "GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Test connection\n",
        "print(\"Testing LLM connection...\")\n",
        "test_response = llm.invoke(\"Hello!\")\n",
        "print(f\"LLM Response: {test_response.content}\")\n",
        "\n",
        "# Define state\n",
        "class MyState(MessagesState):\n",
        "    pass\n",
        "\n",
        "# Database tool\n",
        "@tool\n",
        "def get_data_from_db(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Execute SQL query on configs_strategies table and return CSV results.\n",
        "    Table columns: name, symbol, time_horizon, data_exchange, use_ml, use_patterns, etc.\n",
        "    \"\"\"\n",
        "    print(f\"Executing query: {query}\")\n",
        "\n",
        "    # Get database credentials\n",
        "    host = os.getenv('DB_HOST')\n",
        "    user_name = os.getenv('DB_USER')\n",
        "    password = os.getenv('DB_PASSWORD')\n",
        "    db_name = os.getenv('DB_NAME')\n",
        "\n",
        "    if not all([host, user_name, password, db_name]):\n",
        "        return \"ERROR: Database credentials not set\"\n",
        "\n",
        "    try:\n",
        "        # Create SQLAlchemy engine (no warning)\n",
        "        connection_string = f\"postgresql://{user_name}:{password}@{host}/{db_name}\"\n",
        "        engine = create_engine(connection_string)\n",
        "\n",
        "        # Execute query\n",
        "        df = pd.read_sql(query, engine)\n",
        "        engine.dispose()  # Clean up connection\n",
        "\n",
        "        if df.empty:\n",
        "            return \"No data found\"\n",
        "        else:\n",
        "            return df.to_csv(index=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"ERROR: {str(e)}\"\n",
        "\n",
        "# Setup tools\n",
        "tools = [get_data_from_db]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# Node functions\n",
        "def llm_node(state: MyState) -> dict:\n",
        "    \"\"\"Main LLM node that processes requests and calls tools\"\"\"\n",
        "    print(\"LLM processing request...\")\n",
        "\n",
        "    # Add context about database schema\n",
        "    messages = state[\"messages\"]\n",
        "    if len(messages) == 1:\n",
        "        context = \"\"\"You help users query a PostgreSQL database table called 'configs_strategies'.\n",
        "                     Key columns: name, symbol, time_horizon, data_exchange, use_ml, use_patterns\n",
        "                     Generate correct SQL queries and present results clearly.\"\"\"\n",
        "\n",
        "        enhanced_message = HumanMessage(content=f\"{context}\\n\\nUser request: {messages[0].content}\")\n",
        "        messages = [enhanced_message]\n",
        "\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [*state[\"messages\"], response]}\n",
        "\n",
        "def format_response(state: MyState) -> dict:\n",
        "    \"\"\"Format the final response after tool execution\"\"\"\n",
        "    print(\"Formatting final response...\")\n",
        "\n",
        "    # Find the tool result\n",
        "    messages = state[\"messages\"]\n",
        "    tool_result = None\n",
        "\n",
        "    for msg in reversed(messages):\n",
        "        if hasattr(msg, 'content') and isinstance(msg.content, str):\n",
        "            if ',' in msg.content and not msg.content.startswith('ERROR'):\n",
        "                tool_result = msg.content\n",
        "                break\n",
        "\n",
        "    if tool_result and not tool_result.startswith('ERROR'):\n",
        "        prompt = f\"The database query was successful. Here's the data:\\n\\n{tool_result}\\n\\nPlease summarize this data clearly.\"\n",
        "    else:\n",
        "        prompt = f\"There was an error with the query: {tool_result}\\n\\nPlease explain what went wrong.\"\n",
        "\n",
        "    final_response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    return {\"messages\": [*state[\"messages\"], final_response]}\n",
        "\n",
        "# Build the graph\n",
        "print(\"Building graph...\")\n",
        "builder = StateGraph(MyState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"llm_node\", llm_node)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "builder.add_node(\"format_response\", format_response)\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"llm_node\")\n",
        "builder.add_conditional_edges(\n",
        "    \"llm_node\",\n",
        "    tools_condition,\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        \"__end__\": \"format_response\"\n",
        "    }\n",
        ")\n",
        "builder.add_edge(\"tools\", \"format_response\")\n",
        "builder.add_edge(\"format_response\", END)\n",
        "\n",
        "# Compile graph\n",
        "graph = builder.compile()\n",
        "print(\"Graph built successfully\")\n",
        "\n",
        "# Test the system\n",
        "def run_query(user_request):\n",
        "    \"\"\"Simple function to run a query\"\"\"\n",
        "    print(f\"\\nRunning query: {user_request}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    try:\n",
        "        result = graph.invoke({\"messages\": [HumanMessage(content=user_request)]})\n",
        "        final_message = result[\"messages\"][-1]\n",
        "        print(\"Result:\")\n",
        "        print(final_message.content)\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Test queries\n",
        "    queries = [\n",
        "        \"Get all rows where time_horizon = '4h'\",\n",
        "        \"Show me 5 configurations that use ML\",\n",
        "        \"Count strategies by time_horizon\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        run_query(query)\n",
        "        print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "3RhRn83UtrYC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rSI2vL13zQV6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WO5o91jdziWL"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}